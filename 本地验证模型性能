import pandas as pd
import numpy as np
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
import torch
from torch.utils.data import Dataset
import os

# --- 1. 设置路径 ---
# 注意：这里是你已经训练并保存好的模型路径
FINE_TUNED_MODEL_PATH = "./fine_tuned_model" # 假设你的模型保存在这个目录下
DATA_DIR = "D:/input/glue/SST-2"

# --- 2. 检查GPU并设置设备 ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
if device.type == 'cuda':
    print(f"使用GPU: {torch.cuda.get_device_name(0)}")
    print(f"CUDA版本: {torch.version.cuda}")
    torch.backends.cudnn.benchmark = True  # 启用cuDNN自动优化
else:
    print("警告: 将使用CPU进行评估")

# --- 3. 数据加载函数 ---
def load_sst2_data(file_path):
    try:
        df = pd.read_csv(file_path, sep='\t', encoding='utf-8')
        df = df.dropna()
        return df['sentence'].tolist(), df['label'].tolist()
    except Exception as e:
        print(f"数据加载失败: {e}")
        exit()

# --- 4. 自定义Dataset类 ---
class SST2Dataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

# --- 5. 定义评估指标函数 ---
def compute_metrics(p):
    predictions = np.argmax(p.predictions, axis=1)
    # 对于二分类任务，F1分数可以使用 'binary' 或 'weighted'
    return {
        'accuracy': accuracy_score(p.label_ids, predictions),
        'f1': f1_score(p.label_ids, predictions, average='binary'), # 或 'weighted'
        'precision': precision_score(p.label_ids, predictions, average='binary'),
        'recall': recall_score(p.label_ids, predictions, average='binary'),
    }

# --- 6. 加载数据 ---
print("\n加载评估数据...")
# 对于评估，我们通常使用独立的测试集或开发集。这里我们继续使用dev.tsv作为评估数据。
test_texts, test_labels = load_sst2_data(f'{DATA_DIR}/dev.tsv')

# 如果你想在训练集中划分出的验证集上评估，可以使用：
# train_texts, train_labels = load_sst2_data(f'{DATA_DIR}/train.tsv')
# _, val_texts, _, val_labels = train_test_split(
#     train_texts, train_labels, test_size=0.1, random_state=42
# )
# 使用 val_texts 和 val_labels 来创建 val_dataset

# --- 7. 加载Tokenizer和已训练模型 ---
print("\n加载Tokenizer和已训练模型...")
try:
    # 从保存的模型路径加载Tokenizer和模型
    tokenizer = BertTokenizer.from_pretrained(FINE_TUNED_MODEL_PATH)
    model = BertForSequenceClassification.from_pretrained(FINE_TUNED_MODEL_PATH)
    model.to(device)
    print("已训练模型和Tokenizer加载成功!")
except Exception as e:
    print(f"模型或Tokenizer加载失败: {e}")
    print("请确保在 'FINE_TUNED_MODEL_PATH' 路径下存在完整的模型文件（如pytorch_model.bin和config.json）。")
    exit()

# --- 8. 数据预处理 ---
MAX_LEN = 128
print("\n预处理评估数据...")
eval_encodings = tokenizer(
    test_texts, # 或者 val_texts，取决于你选择哪个作为评估集
    truncation=True,
    padding='max_length',
    max_length=MAX_LEN,
    return_tensors='pt'
)
eval_dataset = SST2Dataset(eval_encodings, test_labels) # 对应 test_labels 或 val_labels

# --- 9. 配置TrainerArguments (仅用于评估) ---
# 评估不需要训练参数，但Trainer仍然需要TrainingArguments实例
training_args = TrainingArguments(
    output_dir='./evaluation_results', # 评估结果的输出目录
    do_eval=True,                      # 明确指定进行评估
    per_device_eval_batch_size=16,     # 每个设备的评估批次大小
    report_to="none",                  # 评估时不需要报告到任何平台
)

# --- 10. 初始化Trainer ---
trainer = Trainer(
    model=model,
    args=training_args,
    eval_dataset=eval_dataset,
    compute_metrics=compute_metrics,
)

# --- 11. 评估模型 ---
print("\n开始评估模型性能...")
results = trainer.evaluate()

print("\n评估结果:")
for key, value in results.items():
    print(f"{key}: {value:.4f}")

print("\n评估完成。")
