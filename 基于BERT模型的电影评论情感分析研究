import pandas as pd
import numpy as np
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import torch
from torch.utils.data import Dataset
import os

# 检查GPU并设置设备
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
if device.type == 'cuda':
    print(f"使用GPU: {torch.cuda.get_device_name(0)}")
    print(f"CUDA版本: {torch.version.cuda}")
    torch.backends.cudnn.benchmark = True  # 启用cuDNN自动优化
else:
    print("警告: 将使用CPU训练")

# 1. 设置路径
DATA_DIR = "D:/input/glue/SST-2"
MODEL_DIR = "D:/input/model/bert-base-uncased"


# 2. 数据加载函数
def load_sst2_data(file_path):
    try:
        df = pd.read_csv(file_path, sep='\t', encoding='utf-8')
        df = df.dropna()
        return df['sentence'].tolist(), df['label'].tolist()
    except Exception as e:
        print(f"数据加载失败: {e}")
        exit()


# 3. 加载数据
print("\n加载数据...")
train_texts, train_labels = load_sst2_data(f'{DATA_DIR}/train.tsv')
test_texts, test_labels = load_sst2_data(f'{DATA_DIR}/dev.tsv')

# 划分训练验证集
train_texts, val_texts, train_labels, val_labels = train_test_split(
    train_texts, train_labels, test_size=0.1, random_state=42
)

# 4. 加载模型
print("\n加载模型...")
try:
    tokenizer = BertTokenizer.from_pretrained(MODEL_DIR)
    model = BertForSequenceClassification.from_pretrained(MODEL_DIR, num_labels=2)
    model.to(device)
    print("模型加载成功!")
except Exception as e:
    print(f"模型加载失败: {e}")
    exit()

# 5. 数据预处理
MAX_LEN = 128


def encode_texts(texts, labels):
    encodings = tokenizer(
        texts,
        truncation=True,
        padding='max_length',
        max_length=MAX_LEN,
        return_tensors='pt'
    )
    return {
        'input_ids': encodings['input_ids'],
        'attention_mask': encodings['attention_mask'],
        'labels': torch.tensor(labels)
    }


print("\n预处理数据...")
train_encodings = encode_texts(train_texts, train_labels)
val_encodings = encode_texts(val_texts, val_labels)
test_encodings = encode_texts(test_texts, test_labels)


# 6. 修改后的Dataset类（解决pin memory问题）
class GLUEDataset(Dataset):
    def __init__(self, encodings):
        self.input_ids = encodings['input_ids']
        self.attention_mask = encodings['attention_mask']
        self.labels = encodings['labels']

    def __getitem__(self, idx):
        return {
            'input_ids': self.input_ids[idx],
            'attention_mask': self.attention_mask[idx],
            'labels': self.labels[idx]
        }

    def __len__(self):
        return len(self.labels)


# 创建数据集（不预先移动到GPU）
train_dataset = GLUEDataset(train_encodings)
val_dataset = GLUEDataset(val_encodings)
test_dataset = GLUEDataset(test_encodings)


# 7. 评估指标
def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    return {'accuracy': accuracy_score(labels, preds)}


# 8. 训练参数优化
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=2,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    learning_rate=3e-5,
    warmup_steps=300,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=100,
    eval_strategy="steps",
    eval_steps=500,
    save_strategy="steps",
    save_steps=500,
    load_best_model_at_end=True,
    fp16=True,
    gradient_accumulation_steps=2,
    optim="adamw_torch_fused",
    report_to="none",
    dataloader_pin_memory=False,  # 关键修改：禁用pin memory
)


# 9. 自定义数据收集函数（解决GPU数据问题）
def custom_collate_fn(batch):
    return {
        'input_ids': torch.stack([x['input_ids'] for x in batch]).to(device),
        'attention_mask': torch.stack([x['attention_mask'] for x in batch]).to(device),
        'labels': torch.stack([x['labels'] for x in batch]).to(device)
    }


# 10. 创建Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
    data_collator=custom_collate_fn  # 使用自定义数据收集
)

# 11. 训练
print("\n开始训练...")
try:
    trainer.train()
    print("训练完成!")
except Exception as e:
    print(f"训练出错: {e}")
    print("尝试减小batch_size或关闭fp16")

# 12. 评估
if trainer.state.best_model_checkpoint:
    print("\n评估最佳模型...")
    eval_results = trainer.evaluate()
    print(f"测试准确率: {eval_results['eval_accuracy']:.4f}")

# 13. 保存模型
model_save_path = "./fine_tuned_model"
model.save_pretrained(model_save_path)
tokenizer.save_pretrained(model_save_path)


# 14. 预测函数
def predict(text):
    inputs = tokenizer(text, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = model(**inputs)
    return "正面" if outputs.logits.argmax().item() == 1 else "负面"


# 测试预测
print("\n测试预测:")
test_texts = [
    "This movie was great!",
    "I hated this film.",
    "The acting was good but the plot was boring"
]
for text in test_texts:
    print(f"文本: {text}")
    print(f"情感: {predict(text)}\n")
